{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+MaN1GmqlAHsw2nxdegjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ansh-Malik1/Conversation-Manager/blob/main/Conversation_Manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugd10UvA42BY",
        "outputId": "f952af34-5c34-4139-9c7f-a7c48df662ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement groq-openai-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for groq-openai-client\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai jsonschema requests groq-openai-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR0-T4dR80tI",
        "outputId": "9a8fc753-b758-44dd-8a03-8e194bfca20e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('GROQ_API_KEY')\n",
        "if api_key:\n",
        "  print(\"API KEY retrieved successfully from secrets\")\n",
        "else:\n",
        "  print(\"Error in retrievin API KEY\")"
      ],
      "metadata": {
        "id": "1DQa7yzWef_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76bdbe6-a3c5-4d8b-ed64-9b6a425c2ef1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API KEY retrieved successfully from secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Task 1 :\n",
        "### 1. To maintain a running conversation history with user-assistant chats\n",
        "### 2. To implement summarization history to keep it concise.\n",
        "### 3. To truncate by last n messages/limit by character or word length\n",
        "### 4. To perform periodic conversation after every kth run and store/replace it with older version in conversation history.\n",
        "### 5. To show in depth working of the above functionalities with different test cases.\n"
      ],
      "metadata": {
        "id": "TASrLikvcPF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation history\n",
        "LLMs are stateless which means they do not remember past messages. LLMs generally rely on tokens which are fed to them and these tokens have a limit too. When the limit is reached, the older messages are dropped in order to accomodate the new ones. Therefore we must maintain a list of messages explicitly.\n",
        "\n",
        "Each message has:\n",
        "\n",
        "*   role - \"user\",\"system\",assistant\"\n",
        "*   content - contains the text\n",
        "\n",
        "Approach I have followed to implement this:\n",
        "\n",
        "*   For storing the history, I will be using python dictionaries. These dictonaries will then be chained together using List data structures.\n",
        "*   I will be creating helper function to add messages to 'history'.\n",
        "*   At later stages, this created 'history' can be sent to LLMs.\n",
        "*   This layer will easily seprate conversational memory with the rest of the logic and will be the fundamental step to the whole assignment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdHbZdJehbD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "from copy import deepcopy\n",
        "from groq import Groq\n",
        "class ConversationManager:\n",
        "  def __init__(self):\n",
        "    self.history : List[Dict[str,str]]= []\n",
        "    self.run_count=0 # Will be used in later stages to perform summarization after every k-th run\n",
        "    self.client=Groq(api_key=api_key)\n",
        "\n",
        "  def add_message(self,role:str,content:str):\n",
        "    assert role in (\"user\",\"assistant\",\"system\")\n",
        "    self.history.append({'role':role,'content':content})\n",
        "\n",
        "  def get_history(self)-> List[Dict[str,str]]:\n",
        "    return deepcopy(self.history)\n",
        "\n",
        "  # code for summarization\n",
        "  def summarize_text(self,text:str)->str:\n",
        "    response=self.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": \"You are an assistant whose goal is to summarize conversation. Keep the contextual meanin intact but compress the conversation\"},\n",
        "            {\"role\":\"user\",\"content\":f\"summarize the following conversation: \\n\\n {text}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "  def summarize_history(self,max_chars=500):\n",
        "    combined = '\\n'.join([f\"{m['role']}: {m['content']}\" for m in self.history])\n",
        "    combined=combined[:max_chars]\n",
        "    summary=self.summarize_text(combined)\n",
        "    self.history = [{'role': 'system', 'content': f\"[SUMMARY]\\n{summary}\"}]\n",
        "    return summary\n",
        "\n",
        "  def periodic_summary(self,k:int):\n",
        "    self.run_count+=1\n",
        "    if k>0 and (self.run_count%k==0):\n",
        "      return self.summarize_history()\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "90KVOPl4ek_C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization\n",
        "If we keep on appending the conversation forever, the database will grow huge taking a toll on memory. Additionaly, the API calls will become expensive and slow due to large amount of tokens being given as input. Thus summarization is required.\n",
        "Summarization compresses history while keeping the contextual meaning intact.\n",
        "\n",
        "Approach:\n",
        "\n",
        "\n",
        "*   Need to keep a track of run_count (already added in the above code)\n",
        "*   The user will provide a parameter 'k'. This parameter will govern after how many runs we need to perform summarization. Thus, when count%k==0, I will perform summarization.\n",
        "*   For summarizatioin, I will be employing Groq.\n",
        "*   After summarization is done, I will be replacing the old messages with the new summarized ones.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ip-0UBprrc1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing cell :\n",
        "cm = ConversationManager()\n",
        "messages=[\"Hello How are you\",\n",
        "          \"I want to learn what is LLM\",\n",
        "          \"How LLM works\",\n",
        "          \"What are some popular LLMs\"\n",
        "]\n",
        "\n",
        "for i, msg in enumerate(messages, start=1):\n",
        "  cm.add_message(\"user\", msg)\n",
        "  response = cm.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=cm.get_history()\n",
        "    )\n",
        "  assistant_reply = response.choices[0].message.content.strip()\n",
        "  cm.add_message(\"assistant\", assistant_reply)\n",
        "  print(f\"\\n--- Turn {i} ---\")\n",
        "  print(\"User:\", msg)\n",
        "  print(\"Assistant:\", assistant_reply)\n",
        "  summary = cm.periodic_summary(k=4)\n",
        "  if summary:\n",
        "    print(\"\\n>>> [SUMMARY TRIGGERED]\")\n",
        "    print(summary)\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "print('Full history ({} messages):'.format(len(cm.get_history())))\n",
        "for m in cm.get_history():\n",
        "    print(m['role'], ':', m['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R6TFlhRqcQg",
        "outputId": "706844ac-d258-406d-80f2-0440478b5922"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 1 ---\n",
            "User: Hello How are you\n",
            "Assistant: I'm doing well, thank you for asking. It's a pleasure to assist you. Is there anything I can help you with or would you like to chat about a particular topic?\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: I want to learn what is LLM\n",
            "Assistant: LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model designed to process and generate human-like language. LLMs are typically trained on massive datasets of text and use this training to predict the next word or phrase in a sentence, essentially \"completing the thought.\"\n",
            "\n",
            "Some key characteristics of LLMs include:\n",
            "\n",
            "1. **Training data**: LLMs are trained on enormous datasets of text, which can range from books and articles to social media posts and online conversations.\n",
            "2. **Language understanding**: LLMs learn to comprehend language patterns, idioms, and nuances, enabling them to generate coherent and contextually relevant text.\n",
            "3. **Predictive capabilities**: LLMs can predict the next word or phrase in a sentence based on the input text, which allows them to generate responses to questions, summarize text, or even create new content.\n",
            "4. **Generative capabilities**: LLMs can use this predictive capability to generate new text, such as answers to questions, chatbot responses, or even entire articles.\n",
            "\n",
            "Some examples of LLMs include:\n",
            "\n",
            "1. **Chatbots**: Many online chatbots, such as customer support bots, use LLMs to understand and respond to user queries.\n",
            "2. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant use LLMs to understand voice commands and respond accordingly.\n",
            "3. **Language translation**: Some language translation software, such as Google Translate, use LLMs to translate text and speech.\n",
            "4. **Content generation**: LLMs are used in content generation tools, such as article writers, to create new content.\n",
            "\n",
            "Some popular types of LLMs include:\n",
            "\n",
            "1. **Transformers**: A family of LLMs developed by the Google research team, which use self-attention mechanisms to process language.\n",
            "2. **BERT (Bidirectional Encoder Representations from Transformers)**: A specific type of LLM developed by Google, which focuses on language understanding and text classification tasks.\n",
            "3. **XLNet (Extreme Language Understanding with a Cross-Modality Topic Model)**: Another prominent LLM developed by the Google research team, which uses bidirectional encoding to improve language understanding.\n",
            "\n",
            "These are just a few examples of the many types of LLMs being developed and used today. The field is rapidly evolving, and new breakthroughs and applications are emerging regularly.\n",
            "\n",
            "Was this explanation helpful? Do you have any specific questions about LLMs?\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: How LLM works\n",
            "Assistant: Let's dive deeper into the inner workings of a Large Language Model (LLM).\n",
            "\n",
            "**Architecture**\n",
            "\n",
            "A typical LLM architecture consists of several key components:\n",
            "\n",
            "1. **Tokenization**: The text input is broken down into individual tokens, such as words, subwords, or characters. This allows the model to process the input at a smaller, more manageable scale.\n",
            "2. **Embedding layer**: Each token is converted into a dense vector representation, known as an embedding. This process is called **embeddings** or **word2vec**.\n",
            "3. **Encoder** (optional): The embeddings are passed through one or more encoder layers, which convert the input into a sequence of vectors that represent the input's meaning and context.\n",
            "4. **Decoding mechanism**: The encoder output is passed through a decoding mechanism, such as a **transformer** or **recurrent neural network (RNN)**, which generates the output text one token at a time.\n",
            "\n",
            "**Key Components:**\n",
            "\n",
            "1. **Self-Attention Mechanism**: The self-attention mechanism allows the model to focus on different parts of the input sequence at different times, generating a weighted sum of the input tokens. This helps the model understand relationships between the tokens.\n",
            "2. **Positional Encoding**: The model uses positional encoding to add information about the token's position in the input sequence. This ensures that the model understands the order and context of the tokens.\n",
            "3. **Feedforward Network**: A feedforward network is used to transform the embeddings into a higher-dimensional space.\n",
            "\n",
            "**Encoder (optional) Components:**\n",
            "\n",
            "1. **Encoder Layers**: These layers are designed to capture both local and global dependencies in the input sequence. They are typically composed of self-attention mechanisms and feedforward networks.\n",
            "2. **Encoder Initializer**: The encoder initializer is used to initialize the weights and bias of the model.\n",
            "\n",
            "**Decoder (optional) Components:**\n",
            "\n",
            "1. **Decoder Layers**: These layers are designed to generate the output sequence. They are typically composed of self-attention mechanisms, feedforward networks, and output layers.\n",
            "2. **Output Layer**: This layer is used to generate the output sequence, one token at a time.\n",
            "\n",
            "**Training Process**\n",
            "\n",
            "1. **Pre-training**: The model is trained on a large dataset of text to learn the relationship between the input and output tokens. This step is also known as **masked language modeling**.\n",
            "2. **Fine-tuning**: The pre-trained model is fine-tuned on a smaller dataset to adjust the model's parameters to the specific task.\n",
            "\n",
            "**Common Training Techniques**\n",
            "\n",
            "1. **Masked Language Modeling**: The input tokens are randomly masked, and the model is trained to predict the original token.\n",
            "2. **Next Sentence Prediction**: The model is trained to predict whether two sentences are adjacent in the original text.\n",
            "3. **Perplexity**: This metric is used to measure how well the model predicts the next token in the input sequence.\n",
            "\n",
            "**Training Algorithms**\n",
            "\n",
            "1. **Stochastic Gradient Descent (SGD)**: This is a common algorithm used to update the model's parameters based on the training objective.\n",
            "2. **Adam**: This algorithm is a variant of SGD that adapts the learning rate for each parameter based on the magnitude of its gradient.\n",
            "\n",
            "**Training Hyperparameters**\n",
            "\n",
            "1. **Batch Size**: This determines how many examples are processed in parallel.\n",
            "2. **Number of Training Iterations**: This determines how many times the model is updated on the training data.\n",
            "3. **Learning Rate**: This determines how quickly the model converges to the optimal solution.\n",
            "\n",
            "The process of training an LLM can be computationally expensive and requires significant resources. However, the results can be impressive, providing state-of-the-art performance on a wide range of natural language processing tasks.\n",
            "\n",
            "Do you have any specific questions about LLMs or the architecture I described?\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: What are some popular LLMs\n",
            "Assistant: Here are some popular Large Language Models (LLMs) that have gained significant attention in the NLP community:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pre-trained model that uses a two-stage approach to learn contextualized representations of input text.\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook AI, RoBERTa is a variant of BERT that improves upon the original model by using a more aggressive training schedule and adding a few new components.\n",
            "3. **DistilBERT (Distilled BERT)**: Developed by Google, DistilBERT is a smaller version of BERT that is designed to be faster and more efficient while still preserving most of the original model's performance.\n",
            "4. **Albert (Conditional BERT)**: Developed by Google, ALBERT is a variant of BERT that uses a different way of representing the input text, which allows it to be more efficient and effective.\n",
            "5. **XLNet (Extreme Language Understanding with a Cross-Modality Topic Model)**: Developed by Google, XLNet is a pre-trained model that uses an autoregressive language model to predict the input text sequence, which allows it to learn more complex language structures.\n",
            "6. **Longformer (Long Document Transformers)**: Developed by Google, Longformer is a variant of BERT that is designed to handle longer documents and can perform tasks that require more context than BERT.\n",
            "7. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a pre-trained model that can perform a wide range of tasks, including text classification, sentiment analysis, and question answering.\n",
            "8. **DeBERTa (Decoding Enhanced Bidirectional Encoder Representations from Transformers)**: Developed by Microsoft, DeBERTa is a variant of BERT that uses a different way of representing the input text, which allows it to be more efficient and effective.\n",
            "9. **SpanBERT (Span-based BERT)**: Developed by Google, SpanBERT is a variant of BERT that focuses on extracting spans of text from a document and using them to predict the task-related information.\n",
            "10. **Flan-T5 (FLAN Text-to-Text Transfer Transformer)**: Developed by Meta AI, Flan-T5 is a variant of T5 that is designed to handle long documents and can perform a wide range of tasks, including text classification, sentiment analysis, and question answering.\n",
            "\n",
            "LLMs have been widely used for a variety of tasks such as:\n",
            "\n",
            "1. **Text Classification**: identifying the category or class of a piece of text (e.g., sentiment analysis, spam detection, etc.)\n",
            "2. **Question Answering**: answering a question based on the information provided in a text\n",
            "3. **Sentiment Analysis**: determining the emotional tone or sentiment of a piece of text (e.g., positive, negative, neutral, etc.)\n",
            "4. **Named Entity Recognition**: identifying and extracting specific entities such as names, locations, and organizations\n",
            "5. **Language Translation**: translating text from one language to another (e.g., English to Spanish, etc.)\n",
            "6. **Text Summarization**: summarizing the main points of a piece of text\n",
            "7. **Text Generation**: generating new text based on a prompt or input (e.g., chatbots, content generation, etc.)\n",
            "\n",
            "These are just a few examples of the many tasks that LLMs can be used for.\n",
            "\n",
            "Do you have any specific questions about these models or their applications?\n",
            "\n",
            ">>> [SUMMARY TRIGGERED]\n",
            "Here's a summary of the conversation:\n",
            "\n",
            "User: Hello, how are you?\n",
            "Me: Doing well, here to assist or chat on a topic.\n",
            "User: Want to learn about LLM (Large Language Model).\n",
            "Me: Explained LLM as a type of AI model designed to process and generate human-like language, trained on large text datasets.\n",
            "-------------------------\n",
            "Full history (1 messages):\n",
            "system : [SUMMARY]\n",
            "Here's a summary of the conversation:\n",
            "\n",
            "User: Hello, how are you?\n",
            "Me: Doing well, here to assist or chat on a topic.\n",
            "User: Want to learn about LLM (Large Language Model).\n",
            "Me: Explained LLM as a type of AI model designed to process and generate human-like language, trained on large text datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H_MW9F3rcgd"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}