{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJdiSr5//m5lZtSFjdw0y+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ansh-Malik1/Conversation-Manager/blob/main/Conversation_Manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugd10UvA42BY",
        "outputId": "f952af34-5c34-4139-9c7f-a7c48df662ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement groq-openai-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for groq-openai-client\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai jsonschema requests groq-openai-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR0-T4dR80tI",
        "outputId": "9a8fc753-b758-44dd-8a03-8e194bfca20e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('GROQ_API_KEY')\n",
        "if api_key:\n",
        "  print(\"API KEY retrieved successfully from secrets\")\n",
        "else:\n",
        "  print(\"Error in retrievin API KEY\")"
      ],
      "metadata": {
        "id": "1DQa7yzWef_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76bdbe6-a3c5-4d8b-ed64-9b6a425c2ef1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API KEY retrieved successfully from secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Task 1 :\n",
        "### 1. To maintain a running conversation history with user-assistant chats\n",
        "### 2. To implement summarization history to keep it concise.\n",
        "### 3. To truncate by last n messages/limit by character or word length\n",
        "### 4. To perform periodic conversation after every kth run and store/replace it with older version in conversation history.\n",
        "### 5. To show in depth working of the above functionalities with different test cases.\n"
      ],
      "metadata": {
        "id": "TASrLikvcPF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation history\n",
        "LLMs are stateless which means they do not remember past messages. LLMs generally rely on tokens which are fed to them and these tokens have a limit too. When the limit is reached, the older messages are dropped in order to accomodate the new ones. Therefore we must maintain a list of messages explicitly.\n",
        "\n",
        "Each message has:\n",
        "\n",
        "*   role - \"user\",\"system\",assistant\"\n",
        "*   content - contains the text\n",
        "\n",
        "Approach I have followed to implement this:\n",
        "\n",
        "*   For storing the history, I will be using python dictionaries. These dictonaries will then be chained together using List data structures.\n",
        "*   I will be creating helper function to add messages to 'history'.\n",
        "*   At later stages, this created 'history' can be sent to LLMs.\n",
        "*   This layer will easily seprate conversational memory with the rest of the logic and will be the fundamental step to the whole assignment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdHbZdJehbD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "from copy import deepcopy\n",
        "from groq import Groq\n",
        "class ConversationManager:\n",
        "  def __init__(self):\n",
        "    self.history : List[Dict[str,str]]= []\n",
        "    self.run_count=0 # Will be used in later stages to perform summarization after every k-th run\n",
        "    self.client=Groq(api_key=api_key)\n",
        "\n",
        "  def add_message(self,role:str,content:str):\n",
        "    assert role in (\"user\",\"assistant\",\"system\")\n",
        "    self.history.append({'role':role,'content':content})\n",
        "\n",
        "  def get_history(self)-> List[Dict[str,str]]:\n",
        "    return deepcopy(self.history)\n",
        "\n",
        "  # code for summarization\n",
        "  def summarize_text(self,text:str)->str:\n",
        "    response=self.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": \"You are an assistant whose goal is to summarize conversation. Keep the contextual meaning intact but compress the conversation. Make sure the summary is in such a way that it cleaerly depicts the conversation uptil now without missing any major points.Dont ask any followup questions. Just take the summary and summarize it.\"},\n",
        "            {\"role\":\"user\",\"content\":f\"summarize the following conversation: \\n\\n {text}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "  def summarize_history(self,max_chars=750):\n",
        "    combined = '\\n'.join([f\"{m['role']}: {m['content']}\" for m in self.history])\n",
        "    combined=combined[:max_chars]\n",
        "    summary=self.summarize_text(combined)\n",
        "    self.history = [{'role': 'system', 'content': f\"[SUMMARY]\\n{summary}\"}]\n",
        "    return summary\n",
        "\n",
        "  def periodic_summary(self,k:int):\n",
        "    self.run_count+=1\n",
        "    if k>0 and (self.run_count%k==0):\n",
        "      return self.summarize_history()\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "90KVOPl4ek_C"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization\n",
        "If we keep on appending the conversation forever, the database will grow huge taking a toll on memory. Additionaly, the API calls will become expensive and slow due to large amount of tokens being given as input. Thus summarization is required.\n",
        "Summarization compresses history while keeping the contextual meaning intact.\n",
        "\n",
        "Approach:\n",
        "\n",
        "\n",
        "*   Need to keep a track of run_count (already added in the above code)\n",
        "*   The user will provide a parameter 'k'. This parameter will govern after how many runs we need to perform summarization. Thus, when count%k==0, I will perform summarization.\n",
        "*   For summarizatioin, I will be employing Groq.\n",
        "*   After summarization is done, I will be replacing the old messages with the new summarized ones.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ip-0UBprrc1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Truncation\n"
      ],
      "metadata": {
        "id": "Wcmaxg5oMqb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing cell :\n",
        "cm = ConversationManager()\n",
        "messages=[\"Hello How are you\",\n",
        "          \"I want to learn what is LLM\",\n",
        "          \"How LLM works\",\n",
        "          \"What are some popular LLMs\"\n",
        "]\n",
        "\n",
        "for i, msg in enumerate(messages, start=1):\n",
        "  cm.add_message(\"user\", msg)\n",
        "  response = cm.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=cm.get_history()\n",
        "    )\n",
        "  assistant_reply = response.choices[0].message.content.strip()\n",
        "  cm.add_message(\"assistant\", assistant_reply)\n",
        "  print(f\"\\n--- Turn {i} ---\")\n",
        "  print(\"User:\", msg)\n",
        "  print(\"Assistant:\", assistant_reply)\n",
        "  summary = cm.periodic_summary(k=2)\n",
        "  if summary:\n",
        "    print(\"\\n>>> [SUMMARY TRIGGERED]\")\n",
        "    print(summary)\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "print('Full history ({} messages):'.format(len(cm.get_history())))\n",
        "for m in cm.get_history():\n",
        "    print(m['role'], ':', m['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R6TFlhRqcQg",
        "outputId": "a4adb94c-d1ac-4bf4-eb29-053aa550a806"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 1 ---\n",
            "User: Hello How are you\n",
            "Assistant: Hello. I'm just a language model, so I don't have emotions or feelings like humans do, but I'm functioning properly and ready to help you with any questions or tasks you may have. How can I assist you today?\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: I want to learn what is LLM\n",
            "Assistant: LLM stands for Large Language Model. It's a type of artificial intelligence (AI) that's been trained on vast amounts of text data to learn patterns, relationships, and structures in language.\n",
            "\n",
            "Large Language Models are typically neural networks that use natural language processing (NLP) techniques to analyze and generate human-like text. They're trained on massive datasets of text, which allows them to learn the nuances of language and generate coherent, context-dependent responses.\n",
            "\n",
            "Some of the key characteristics of LLMs include:\n",
            "\n",
            "1. **Training data**: LLMs are trained on vast amounts of text data, often sourced from the internet, books, and other sources.\n",
            "2. **Neural networks**: LLMs are implemented using neural networks, which enable them to learn complex patterns and relationships in language.\n",
            "3. **Self-supervised learning**: LLMs are trained using self-supervised learning techniques, where the model generates text and then tries to predict the next word or phrase.\n",
            "4. **Generative capabilities**: LLMs can generate coherent text based on the input they receive, making them useful for tasks like language translation, text summarization, and chatbots.\n",
            "5. **Contextual understanding**: LLMs are capable of understanding context and nuances of language, making them more accurate and human-like in their responses.\n",
            "\n",
            "Some of the applications of LLMs include:\n",
            "\n",
            "1. **Language translation**\n",
            "2. **Text summarization**\n",
            "3. **Chatbots and conversational AI**\n",
            "4. **Language modeling**\n",
            "5. **Content generation**\n",
            "\n",
            "Examples of popular LLMs include:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**\n",
            "3. **XLNet (Extreme Language Modeling and Inductive Reasoning)**\n",
            "\n",
            "LLMs have many potential applications in fields like:\n",
            "\n",
            "1. **Natural language processing (NLP)**\n",
            "2. **Human-computer interaction**\n",
            "3. **Content generation**\n",
            "4. **Language learning**\n",
            "\n",
            "I hope this gives you a good introduction to the world of LLMs!\n",
            "\n",
            ">>> [SUMMARY TRIGGERED]\n",
            "Here's a compressed summary of the conversation:\n",
            "\n",
            "A user initiated a conversation by asking \"Hello How are you\" and the assistant responded, explaining it doesn't have emotions but is ready to help. The user then asked about LLM (Large Language Model), and the assistant provided a definition: it's a type of AI trained on vast amounts of text data to learn language patterns and generate human-like text.\n",
            "-------------------------\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: How LLM works\n",
            "Assistant: I'd be happy to explain how LLMs (Large Language Models) work.\n",
            "\n",
            "**Overview**\n",
            "\n",
            "LLMs are a type of deep learning model that uses a type of artificial neural network to process and generate human-like text. They are trained on vast amounts of text data, often sourced from the internet, books, and other written resources.\n",
            "\n",
            "**Architecture**\n",
            "\n",
            "The architecture of an LLM typically consists of several layers:\n",
            "\n",
            "1. **Input Layer**: This layer takes in raw text data, such as a sentence or a paragraph.\n",
            "2. **Embedding Layer**: This layer converts the input text into numerical representations, called embeddings, that can be processed by the model.\n",
            "3. **Encoder Layers**: These layers are where the magic happens. They use complex algorithms to analyze the embeddings and learn patterns, relationships, and semantics in the language.\n",
            "4. **Decoder Layers**: These layers take the output from the encoder layers and generate new text based on the patterns learned during training.\n",
            "5. **Output Layer**: This layer produces the final text output.\n",
            "\n",
            "**Training**\n",
            "\n",
            "To train an LLM, the model is fed a massive corpus of text data, which it uses to learn patterns, relationships, and semantics. The model is trained using a variety of techniques, including:\n",
            "\n",
            "1. **Masked Language Modeling**: The model is given a piece of text with some words replaced with a mask token. The model must predict the missing word(s).\n",
            "2. **Next Sentence Prediction**: The model is given two sentences and must predict whether the second sentence is a continuation of the first sentence or not.\n",
            "3. **Other tasks**: Depending on the specific model, other tasks such as language translation, question answering, or text classification may be included.\n",
            "\n",
            "**How LLMs generate text**\n",
            "\n",
            "When given a prompt or input text, the LLM generates text by:\n",
            "\n",
            "1. **Creating a contextualized representation**: The model creates a complex representation of the input text, taking into account the words, phrases, and context.\n",
            "2. **Predicting the next token**: The model predicts the next token (word, character, etc.) in the sequence based on the contextualized representation.\n",
            "3. **Iterating**: The model iteratively predicts the next token until it reaches the end of the sequence or a specified length.\n",
            "\n",
            "**Challenges**\n",
            "\n",
            "Training LLMs is a complex task that involves:\n",
            "\n",
            "1. **Scalability**: LLMs require massive amounts of computational resources and large-scale text datasets.\n",
            "2. **Overfitting**: LLMs can overfit to the training data, resulting in poor performance on new, unseen data.\n",
            "3. **Bias and fairness**: LLMs can perpetuate biases and prejudices present in the training data.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "LLMs are powerful tools for natural language processing and generation, but they are not without their challenges. As research and development continue, LLMs are expected to become increasingly sophisticated and useful in a wide range of applications.\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: What are some popular LLMs\n",
            "Assistant: Here are some popular LLMs (Large Language Models):\n",
            "\n",
            "**1. BERT (Bidirectional Encoder Representations from Transformers)**\n",
            "\n",
            "* Developed by Google in 2018\n",
            "* Trained on a large corpus of text data (up to 3.3 billion parameters)\n",
            "* Achieved state-of-the-art results in many natural language processing tasks\n",
            "* Used in various applications, including language translation, text classification, and question answering\n",
            "\n",
            "**2. RoBERTa (Robustly Optimized BERT Pretraining Approach)**\n",
            "\n",
            "* Developed by Facebook in 2019\n",
            "* Trained on an even larger corpus of text data (up to 160 GB) than BERT\n",
            "* Achieved better results than BERT in many tasks\n",
            "* Used in various applications, including language translation, text classification, and sentiment analysis\n",
            "\n",
            "**3. T5 (Text-to-Text Transfer Transformer)**\n",
            "\n",
            "* Developed by Google in 2020\n",
            "* Trained on a large corpus of text data (up to 2 billion parameters)\n",
            "* Designed to be a general-purpose text-to-text model\n",
            "* Achieved state-of-the-art results in various tasks, including language translation and text summarization\n",
            "\n",
            "**4. Longformer (Self-Attention Neural Networks for Language Tasks)**\n",
            "\n",
            "* Developed by Facebook in 2020\n",
            "* Designed to handle long-range dependencies in text data\n",
            "* Trained on a large corpus of text data (up to 2.5 billion parameters)\n",
            "* Achieved state-of-the-art results in tasks such as question answering and text classification\n",
            "\n",
            "**5. XLNet (Generalized Autoregressive Pretraining for Language)**\n",
            "\n",
            "* Developed by Google in 2019\n",
            "* Designed to be more flexible and robust than BERT\n",
            "* Trained on a large corpus of text data (up to 1.5 billion parameters)\n",
            "* Achieved state-of-the-art results in various tasks, including language translation and text classification\n",
            "\n",
            "**6. Transformers (Attention is All You Need)**\n",
            "\n",
            "* Developed by Google in 2017 (later improved and scaled up)\n",
            "* Designed to be a general-purpose model for natural language processing tasks\n",
            "* Trained on a large corpus of text data (up to 1.5 billion parameters)\n",
            "* Achieved state-of-the-art results in various tasks, including language translation and text classification\n",
            "\n",
            "**7. DistilBERT (Distilled BERT for Natural Language Processing)**\n",
            "\n",
            "* Developed by Hugging Face in 2020\n",
            "* Designed to be a more efficient and smaller version of BERT\n",
            "* Trained on a large corpus of text data (up to 300 million parameters)\n",
            "* Achieved competitive results to BERT in various tasks\n",
            "\n",
            "**8. ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)**\n",
            "\n",
            "* Developed by Google in 2020\n",
            "* Designed to be a more efficient and scalable version of BERT\n",
            "* Trained on a large corpus of text data (up to 2.6 billion parameters)\n",
            "* Achieved competitive results to BERT in various tasks\n",
            "\n",
            "**9. Megatron-LM (A Massive Parallel Model for Natural Language Processing)**\n",
            "\n",
            "* Developed by Facebook in 2020\n",
            "* Designed to be a highly scalable and parallelizable version of BERT\n",
            "* Trained on a massive corpus of text data (up to 10 billion parameters)\n",
            "* Achieved competitive results to BERT in various tasks\n",
            "\n",
            "**10. LaMDA (Large Language Model Dialogue Application)**\n",
            "\n",
            "* Developed by Google in 2021\n",
            "* Designed to be a conversational AI system that can engage in natural-sounding conversations\n",
            "* Trained on a massive corpus of text data (up to 137 billion parameters)\n",
            "* Currently in development and not yet publicly available\n",
            "\n",
            "These are just a few examples of popular LLMs. There are many other models and variants being developed and researched in the field of natural language processing.\n",
            "\n",
            ">>> [SUMMARY TRIGGERED]\n",
            "Here's a compressed summary of the conversation:\n",
            "\n",
            "The conversation started with a user asking \"Hello How are you\" and the assistant explaining it's an emotionless being ready to help. The user then asked about LLM (Large Language Model), which the assistant defined as a type of AI trained on vast text data to generate human-like text. The user asked \"How LLM works\", and the assistant provided an overview, explaining that LLMs use artificial neural networks and are trained on vast amounts of text data, sourced from the internet, books, and more.\n",
            "-------------------------\n",
            "Full history (1 messages):\n",
            "system : [SUMMARY]\n",
            "Here's a compressed summary of the conversation:\n",
            "\n",
            "The conversation started with a user asking \"Hello How are you\" and the assistant explaining it's an emotionless being ready to help. The user then asked about LLM (Large Language Model), which the assistant defined as a type of AI trained on vast text data to generate human-like text. The user asked \"How LLM works\", and the assistant provided an overview, explaining that LLMs use artificial neural networks and are trained on vast amounts of text data, sourced from the internet, books, and more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H_MW9F3rcgd"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}