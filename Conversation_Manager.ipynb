{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLhsOggnIVvRKltVg6NST3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ansh-Malik1/Conversation-Manager/blob/main/Conversation_Manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugd10UvA42BY",
        "outputId": "58d21b81-58a7-40f6-fc60-019218bef5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement groq-openai-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for groq-openai-client\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai jsonschema requests groq-openai-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR0-T4dR80tI",
        "outputId": "97758151-9d83-448d-b043-d81de96495a3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('GROQ_API_KEY')\n",
        "if api_key:\n",
        "  print(\"API KEY retrieved successfully from secrets\")\n",
        "else:\n",
        "  print(\"Error in retrievin API KEY\")"
      ],
      "metadata": {
        "id": "1DQa7yzWef_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b11dbf-e9d6-43c5-c5d5-31b44b8293bc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API KEY retrieved successfully from secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Task 1 :\n",
        "### 1. To maintain a running conversation history with user-assistant chats\n",
        "### 2. To implement summarization history to keep it concise.\n",
        "### 3. To truncate by last n messages/limit by character or word length\n",
        "### 4. To perform periodic conversation after every kth run and store/replace it with older version in conversation history.\n",
        "### 5. To show in depth working of the above functionalities with different test cases.\n"
      ],
      "metadata": {
        "id": "TASrLikvcPF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation history\n",
        "LLMs are stateless which means they do not remember past messages. LLMs generally rely on tokens which are fed to them and these tokens have a limit too. When the limit is reached, the older messages are dropped in order to accomodate the new ones. Therefore we must maintain a list of messages explicitly.\n",
        "\n",
        "Each message has:\n",
        "\n",
        "*   role - \"user\",\"system\",assistant\"\n",
        "*   content - contains the text\n",
        "\n",
        "Approach I have followed to implement this:\n",
        "\n",
        "*   For storing the history, I will be using python dictionaries. These dictonaries will then be chained together using List data structures.\n",
        "*   I will be creating helper function to add messages to 'history'.\n",
        "*   At later stages, this created 'history' can be sent to LLMs.\n",
        "*   This layer will easily seprate conversational memory with the rest of the logic and will be the fundamental step to the whole assignment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdHbZdJehbD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "from copy import deepcopy\n",
        "from groq import Groq\n",
        "class ConversationManager:\n",
        "  def __init__(self):\n",
        "    self.history : List[Dict[str,str]]= []\n",
        "    self.run_count=0 # Will be used in later stages to perform summarization after every k-th run\n",
        "    self.client=Groq(api_key=api_key)\n",
        "\n",
        "  def add_message(self,role:str,content:str):\n",
        "    assert role in (\"user\",\"assistant\",\"system\")\n",
        "    self.history.append({'role':role,'content':content})\n",
        "\n",
        "  def get_history(self)-> List[Dict[str,str]]:\n",
        "    return deepcopy(self.history)\n",
        "\n",
        "  # code for summarization\n",
        "  def summarize_text(self,text:str)->str:\n",
        "    response=self.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": \"You are an assistant whose goal is to summarize conversation. Keep the contextual meaning intact but compress the conversation. Make sure the summary is in such a way that it cleaerly depicts the conversation uptil now without missing any major points.Dont ask any followup questions. Just take the summary and summarize it.\"},\n",
        "            {\"role\":\"user\",\"content\":f\"summarize the following conversation: \\n\\n {text}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "  def summarize_history(self,max_chars=750):\n",
        "    combined = '\\n'.join([f\"{m['role']}: {m['content']}\" for m in self.history])\n",
        "    combined=combined[:max_chars]\n",
        "    summary=self.summarize_text(combined)\n",
        "    self.history = [{'role': 'system', 'content': f\"[SUMMARY]\\n{summary}\"}]\n",
        "    return summary\n",
        "\n",
        "  def periodic_summary(self,k:int):\n",
        "    self.run_count+=1\n",
        "    if k>0 and (self.run_count%k==0):\n",
        "      return self.summarize_history()\n",
        "    return None\n",
        "\n",
        "  def truncate_by_turns(self,n:int)-> List[Dict[str,str]]:\n",
        "    if n<=0:\n",
        "      return []\n",
        "    return deepcopy(self.history[-n:])\n",
        "\n",
        "  def truncate_by_chars(self,max_chars:int)-> List[Dict[str,str]]:\n",
        "    if max_chars<=0:\n",
        "      return []\n",
        "    kept=[]\n",
        "    total=0\n",
        "    for msg in reversed(self.history):\n",
        "      l=len(msg['content'])\n",
        "      if total+l<=max_chars:\n",
        "        kept.insert(0,msg)\n",
        "        total+=l\n",
        "      else:\n",
        "        remaining=max_chars-total\n",
        "        if remaining>0:\n",
        "          kept.insert(0,{\n",
        "              \"role\":msg[\"role\"],\n",
        "              \"content\":msg[\"content\"][-remaining:]\n",
        "          })\n",
        "        break\n",
        "    return list(reversed(deepcopy(kept)))\n",
        "\n",
        "  def truncate_by_words(self,max_words:int)-> List[Dict[str,str]]:\n",
        "    if max_words<=0:\n",
        "      return []\n",
        "    kept=[]\n",
        "    total=0\n",
        "    for msg in reversed(self.history):\n",
        "      words = msg[\"content\"].split()\n",
        "      w=len(msg['content'].split())\n",
        "      if total+w<=max_words:\n",
        "        kept.insert(0,msg )\n",
        "        total+=w\n",
        "      else:\n",
        "        remaining=max_words-total\n",
        "        if remaining>0:\n",
        "          kept.insert(0,{\n",
        "              \"role\":msg[\"role\"],\n",
        "              \"content\":\" \".join(words[-remaining:])\n",
        "          })\n",
        "        break\n",
        "    return list(reversed(deepcopy(kept)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "90KVOPl4ek_C"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization\n",
        "If we keep on appending the conversation forever, the database will grow huge taking a toll on memory. Additionaly, the API calls will become expensive and slow due to large amount of tokens being given as input. Thus summarization is required.\n",
        "Summarization compresses history while keeping the contextual meaning intact.\n",
        "\n",
        "Approach:\n",
        "\n",
        "\n",
        "*   Need to keep a track of run_count (already added in the above code)\n",
        "*   The user will provide a parameter 'k'. This parameter will govern after how many runs we need to perform summarization. Thus, when count%k==0, I will perform summarization.\n",
        "*   For summarizatioin, I will be employing Groq.\n",
        "*   After summarization is done, I will be replacing the old messages with the new summarized ones.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ip-0UBprrc1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Truncation\n",
        "Truncation refers to removing/omitting some part of the word or sentence we are dealing with. Even after summarization, we sometimes require to truncate the history for further increase in effeciency.\n",
        "\n",
        "Approach:\n",
        "Will be using 3 seprate functions for different types of truncations which are: truncation by characters, truncation by words and lastly, truncating and keeping last 'n' messages."
      ],
      "metadata": {
        "id": "Wcmaxg5oMqb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing cell :\n",
        "cm = ConversationManager()\n",
        "messages=[\"Hello How are you\",\n",
        "          \"I want to learn what is LLM\",\n",
        "          \"How LLM works\",\n",
        "          \"What are some popular LLMs\"\n",
        "]\n",
        "\n",
        "for i, msg in enumerate(messages, start=1):\n",
        "  cm.add_message(\"user\", msg)\n",
        "  response = cm.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=cm.get_history()\n",
        "    )\n",
        "  assistant_reply = response.choices[0].message.content.strip()\n",
        "  cm.add_message(\"assistant\", assistant_reply)\n",
        "  # print(f\"\\n--- Turn {i} ---\")\n",
        "  # print(\"User:\", msg)\n",
        "  # print(\"Assistant:\", assistant_reply)\n",
        "  # summary = cm.periodic_summary(k=2)\n",
        "  # if summary:\n",
        "    # print(\"\\n>>> [SUMMARY TRIGGERED]\")\n",
        "    # print(summary)\n",
        "    # print(\"-------------------------\")\n",
        "\n",
        "#print('Full history ({} messages):'.format(len(cm.get_history())))\n",
        "# for m in cm.get_history():\n",
        "    # print(m['role'], ':', m['content'])\n",
        "\n",
        "#print(\"Full history:\", cm.history)\n",
        "print(\"Truncated by last 30 chars:\", cm.truncate_by_chars(30))\n",
        "print(\"Truncated by 10 words:\", cm.truncate_by_words(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R6TFlhRqcQg",
        "outputId": "d14f7467-e1d6-4e64-a1be-17ed7b1b323c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncated by last 30 chars: [{'role': 'assistant', 'content': 'eloped and released regularly.'}]\n",
            "Truncated by 10 words: [{'role': 'assistant', 'content': 'list, and new LLMs are being developed and released regularly.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H_MW9F3rcgd"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}