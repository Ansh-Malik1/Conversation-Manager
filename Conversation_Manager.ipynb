{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNExwgR84V/ZkfM0eTAQiWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ansh-Malik1/Conversation-Manager/blob/main/Conversation_Manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugd10UvA42BY",
        "outputId": "58d21b81-58a7-40f6-fc60-019218bef5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement groq-openai-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for groq-openai-client\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai jsonschema requests groq-openai-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR0-T4dR80tI",
        "outputId": "97758151-9d83-448d-b043-d81de96495a3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('GROQ_API_KEY')\n",
        "if api_key:\n",
        "  print(\"API KEY retrieved successfully from secrets\")\n",
        "else:\n",
        "  print(\"Error in retrievin API KEY\")"
      ],
      "metadata": {
        "id": "1DQa7yzWef_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b11dbf-e9d6-43c5-c5d5-31b44b8293bc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API KEY retrieved successfully from secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Task 1 :\n",
        "### 1. To maintain a running conversation history with user-assistant chats\n",
        "### 2. To implement summarization history to keep it concise.\n",
        "### 3. To truncate by last n messages/limit by character or word length\n",
        "### 4. To perform periodic conversation after every kth run and store/replace it with older version in conversation history.\n",
        "### 5. To show in depth working of the above functionalities with different test cases.\n"
      ],
      "metadata": {
        "id": "TASrLikvcPF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation history\n",
        "LLMs are stateless which means they do not remember past messages. LLMs generally rely on tokens which are fed to them and these tokens have a limit too. When the limit is reached, the older messages are dropped in order to accomodate the new ones. Therefore we must maintain a list of messages explicitly.\n",
        "\n",
        "Each message has:\n",
        "\n",
        "*   role - \"user\",\"system\",assistant\"\n",
        "*   content - contains the text\n",
        "\n",
        "Approach I have followed to implement this:\n",
        "\n",
        "*   For storing the history, I will be using python dictionaries. These dictonaries will then be chained together using List data structures.\n",
        "*   I will be creating helper function to add messages to 'history'.\n",
        "*   At later stages, this created 'history' can be sent to LLMs.\n",
        "*   This layer will easily seprate conversational memory with the rest of the logic and will be the fundamental step to the whole assignment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdHbZdJehbD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "from copy import deepcopy\n",
        "from groq import Groq\n",
        "class ConversationManager:\n",
        "  def __init__(self):\n",
        "    self.history : List[Dict[str,str]]= []\n",
        "    self.run_count=0 # Will be used in later stages to perform summarization after every k-th run\n",
        "    self.client=Groq(api_key=api_key)\n",
        "\n",
        "  def add_message(self,role:str,content:str):\n",
        "    assert role in (\"user\",\"assistant\",\"system\")\n",
        "    self.history.append({'role':role,'content':content})\n",
        "\n",
        "  def get_history(self)-> List[Dict[str,str]]:\n",
        "    return deepcopy(self.history)\n",
        "\n",
        "  # code for summarization\n",
        "  def summarize_text(self,text:str)->str:\n",
        "    response=self.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": \"You are an assistant whose goal is to summarize conversation. Keep the contextual meaning intact but compress the conversation. Make sure the summary is in such a way that it cleaerly depicts the conversation uptil now without missing any major points.Dont ask any followup questions. Just take the summary and summarize it.\"},\n",
        "            {\"role\":\"user\",\"content\":f\"summarize the following conversation: \\n\\n {text}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "  def summarize_history(self,max_chars=750):\n",
        "    combined = '\\n'.join([f\"{m['role']}: {m['content']}\" for m in self.history])\n",
        "    combined=combined[:max_chars]\n",
        "    summary=self.summarize_text(combined)\n",
        "    self.history = [{'role': 'system', 'content': f\"[SUMMARY]\\n{summary}\"}]\n",
        "    return summary\n",
        "\n",
        "  def periodic_summary(self,k:int):\n",
        "    self.run_count+=1\n",
        "    if k>0 and (self.run_count%k==0):\n",
        "      return self.summarize_history()\n",
        "    return None\n",
        "\n",
        "  def truncate_by_turns(self,n:int)-> List[Dict[str,str]]:\n",
        "    if n<=0:\n",
        "      return []\n",
        "    return deepcopy(self.history[-n:])\n",
        "\n",
        "  def truncate_by_chars(self,max_chars:int)-> List[Dict[str,str]]:\n",
        "    if max_chars<=0:\n",
        "      return []\n",
        "    kept=[]\n",
        "    total=0\n",
        "    for msg in reversed(self.history):\n",
        "      l=len(msg['content'])\n",
        "      if total+l<=max_chars:\n",
        "        kept.insert(0,msg)\n",
        "        total+=l\n",
        "      else:\n",
        "        remaining=max_chars-total\n",
        "        if remaining>0:\n",
        "          kept.insert(0,{\n",
        "              \"role\":msg[\"role\"],\n",
        "              \"content\":msg[\"content\"][-remaining:]\n",
        "          })\n",
        "        break\n",
        "    return list(reversed(deepcopy(kept)))\n",
        "\n",
        "  def truncate_by_words(self,max_words:int)-> List[Dict[str,str]]:\n",
        "    if max_words<=0:\n",
        "      return []\n",
        "    kept=[]\n",
        "    total=0\n",
        "    for msg in reversed(self.history):\n",
        "      words = msg[\"content\"].split()\n",
        "      w=len(msg['content'].split())\n",
        "      if total+w<=max_words:\n",
        "        kept.insert(0,msg )\n",
        "        total+=w\n",
        "      else:\n",
        "        remaining=max_words-total\n",
        "        if remaining>0:\n",
        "          kept.insert(0,{\n",
        "              \"role\":msg[\"role\"],\n",
        "              \"content\":\" \".join(words[-remaining:])\n",
        "          })\n",
        "        break\n",
        "    return list(reversed(deepcopy(kept)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "90KVOPl4ek_C"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization\n",
        "If we keep on appending the conversation forever, the database will grow huge taking a toll on memory. Additionaly, the API calls will become expensive and slow due to large amount of tokens being given as input. Thus summarization is required.\n",
        "Summarization compresses history while keeping the contextual meaning intact.\n",
        "\n",
        "Approach:\n",
        "\n",
        "\n",
        "*   Need to keep a track of run_count (already added in the above code)\n",
        "*   The user will provide a parameter 'k'. This parameter will govern after how many runs we need to perform summarization. Thus, when count%k==0, I will perform summarization.\n",
        "*   For summarizatioin, I will be employing Groq.\n",
        "*   After summarization is done, I will be replacing the old messages with the new summarized ones.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ip-0UBprrc1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Truncation\n",
        "Truncation refers to removing/omitting some part of the word or sentence we are dealing with. Even after summarization, we sometimes require to truncate the history for further increase in effeciency.\n",
        "\n",
        "Approach:\n",
        "Will be using 3 seprate functions for different types of truncations which are: truncation by characters, truncation by words and lastly, truncating and keeping last 'n' messages."
      ],
      "metadata": {
        "id": "Wcmaxg5oMqb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing cell :\n",
        "cm = ConversationManager()\n",
        "messages=[\"Hello How are you\",\n",
        "          \"I want to learn what is LLM\",\n",
        "          \"How LLM works\",\n",
        "          \"What are some popular LLMs\"\n",
        "]\n",
        "\n",
        "for i, msg in enumerate(messages, start=1):\n",
        "  cm.add_message(\"user\", msg)\n",
        "  response = cm.client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=cm.get_history()\n",
        "    )\n",
        "  assistant_reply = response.choices[0].message.content.strip()\n",
        "  cm.add_message(\"assistant\", assistant_reply)\n",
        "  # print(f\"\\n--- Turn {i} ---\")\n",
        "  # print(\"User:\", msg)\n",
        "  # print(\"Assistant:\", assistant_reply)\n",
        "  # summary = cm.periodic_summary(k=2)\n",
        "  # if summary:\n",
        "    # print(\"\\n>>> [SUMMARY TRIGGERED]\")\n",
        "    # print(summary)\n",
        "    # print(\"-------------------------\")\n",
        "\n",
        "#print('Full history ({} messages):'.format(len(cm.get_history())))\n",
        "# for m in cm.get_history():\n",
        "    # print(m['role'], ':', m['content'])\n",
        "\n",
        "print(\"Full history:\", cm.history)\n",
        "print(\"Truncated by last 3 chars:\", cm.truncate_by_chars(30))\n",
        "print(\"Truncated by 10 words:\", cm.truncate_by_words(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R6TFlhRqcQg",
        "outputId": "784c11df-8b98-4711-bba9-b404c5bda72b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full history: [{'role': 'user', 'content': 'Hello How are you'}, {'role': 'assistant', 'content': \"Hello. I'm functioning properly. What would you like to talk about or ask today?\"}, {'role': 'user', 'content': 'I want to learn what is LLM'}, {'role': 'assistant', 'content': 'LLM stands for Large Language Model. It is a type of artificial intelligence (AI) model that is trained on a massive corpus of text data, enabling it to generate human-like responses to a wide range of questions and prompts.\\n\\nLarge Language Models are typically trained using a type of machine learning technique called deep learning, specifically transformer architectures. These models are trained on vast amounts of text data, which allows them to learn patterns, relationships, and context within language.\\n\\nThe key characteristics of Large Language Models are:\\n\\n1. **Language Understanding**: They can comprehend and analyze vast amounts of text data.\\n2. **Contextual Understanding**: They can understand the nuances of language, such as idioms, irony, and figurative language.\\n3. **Generate Human-like Responses**: They can respond to questions and prompts in a way that is similar to human writing.\\n4. **Conversational Understanding**: They can engage in conversation, understand context, and respond accordingly.\\n\\nSome popular examples of Large Language Models include:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: This is one of the pioneering models for Large Language Models.\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: An improved version of BERT.\\n3. ** transformer-XL**: This model uses a more complex architecture to capture long-range dependencies in the text.\\n4. **LLaMA**: A more recent Large Language Model that has achieved state-of-the-art results on many natural language processing tasks.\\n\\nThese models have numerous applications, such as:\\n\\n1. **Natural Language Processing (NLP)**: They can help with tasks like sentiment analysis, text classification, and language translation.\\n2. **Chatbots**: They can power chatbots that can engage in conversation with humans.\\n3. **Content Generation**: They can generate text, like articles, stories, or even entire books.\\n\\nOverall, Large Language Models are a significant step forward in the field of AI, and their applications continue to grow and improve.'}, {'role': 'user', 'content': 'How LLM works'}, {'role': 'assistant', 'content': 'Large Language Models (LLM) work by analyzing massive amounts of text data and learning patterns, relationships, and context within language. Here\\'s a simplified overview of how LLM works:\\n\\n**1. Data Collection**:\\nThe training process starts with collecting a massive corpus of text data, which can include books, articles, research papers, websites, and more. This dataset is preprocessed to remove irrelevant information, such as HTML tags, dates, and names.\\n\\n**2. Tokenization**:\\nThe text data is then tokenized, which means breaking it down into smaller units, such as individual words or subwords. These tokens are then used as input to the model.\\n\\n**3. Embedding**:\\nThe tokens are then turned into numerical representations called embeddings, which capture their meaning and relationships. These embeddings are the foundation of the model\\'s understanding of language.\\n\\n**4. Masking**:\\nSome of the input tokens are randomly replaced with a special token called a \"[MASK]\" token. This is done to simulate real-world language understanding, where words may be missing or unknown.\\n\\n**5. Encoder**:\\nThe input tokens, including the masked tokens, are fed through a series of layers called the encoder. Each layer applies a self-attention mechanism, which allows the model to weigh the importance of each token relative to the others.\\n\\n**6. Decoder**:\\nThe output from the encoder is then fed through a decoder, which generates a sequence of tokens that correspond to the input text. The decoder is a variant of the encoder, but with a different architecture.\\n\\n**7. Training**:\\nThe model is trained using a loss function that measures the difference between the predicted output and the actual output. The model is tuned to minimize this difference.\\n\\n**8. Inference**:\\nOnce the model is trained, it can be used to generate text based on a given input, such as a prompt or a question.\\n\\n**Architecture**:\\nLarge Language Models typically use a variant of the Transformer architecture, which consists of two main components:\\n\\n* **Encoder**: The encoder takes in a sequence of tokens and produces a sequence of vectors that represent the input.\\n* **Decoder**: The decoder takes in the output from the encoder and generates a sequence of tokens that correspond to the input text.\\n\\nSome of the key components of the Transformer architecture include:\\n\\n* **Self-Attention**: This mechanism allows the model to weigh the importance of each token relative to the others.\\n* **Positional Encoding**: This mechanism helps the model understand the order of the input tokens.\\n* **Transformer Encoder**: This layer applies the self-attention mechanism to the input tokens.\\n* **Transformer Decoder**: This layer generates the output tokens based on the input from the encoder.\\n\\n**Key Techniques**:\\nLarge Language Models rely on several key techniques to achieve their performance:\\n\\n* **Pre-training**: This involves training the model on a large corpus of text data to learn patterns and relationships.\\n* **Fine-tuning**: This involves adapting the model to a specific task, such as question-answering or document summarization.\\n* **Multi-task Learning**: This involves training the model on multiple tasks simultaneously to improve its overall performance.\\n\\nOverall, Large Language Models work by leveraging a combination of techniques, including pre-training, fine-tuning, and multi-task learning, to learn patterns, relationships, and context within language.'}, {'role': 'user', 'content': 'What are some popular LLMs'}, {'role': 'assistant', 'content': 'Here are some popular Large Language Models (LLMs):\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**:\\n\\t* Developed by Google in 2018\\n\\t* Pre-trained on a large corpus of text data (BookCorpus and Wikipedia)\\n\\t* Fine-tuned for various NLP tasks, such as question-answering and sentiment analysis\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**:\\n\\t* Developed by Facebook in 2019\\n\\t* Builds upon BERT and improves upon it by using a different training objective and a longer input context\\n\\t* Achieved state-of-the-art results on several NLP tasks\\n3. ** transformer-XL**:\\n\\t* Developed by Google and Carnegie Mellon University in 2019\\n\\t* Designed to address the limitations of BERT and RoBERTa by using a more complex architecture and longer input context\\n\\t* Achieved state-of-the-art results on several NLP tasks\\n4. **DistilBERT**:\\n\\t* Developed by Google in 2019\\n\\t* A compressed version of BERT that achieves similar performance at a fraction of the cost and computational resources\\n\\t* Designed for deployment in production environments\\n5. **ALBERT (A Lite BERT)**:\\n\\t* Developed by Google in 2019\\n\\t* A lightweight version of BERT that achieves similar performance at a fraction of the cost and computational resources\\n\\t* Designed for deployment in production environments\\n6. **Longformer**:\\n\\t* Developed by Facebook AI in 2020\\n\\t* A new type of transformer architecture that is designed to handle long-range dependencies\\n\\t* Achieved state-of-the-art results on several NLP tasks\\n7. **T5 (Text-to-Text Transfer Transformer)**:\\n\\t* Developed by Google in 2020\\n\\t* A multi-task learning model that can perform a wide range of NLP tasks, such as text classification and question-answering\\n\\t* Achieved state-of-the-art results on several NLP tasks\\n8. **Flan**:\\n\\t* Developed by Google in 2019\\n\\t* A multi-lingual version of BERT that can handle text data in multiple languages\\n\\t* Achieved state-of-the-art results on several NLP tasks for multiple languages\\n9. **XLM-R (Cross-Lingual Language Model)**:\\n\\t* Developed by Carnegie Mellon University and Facebook AI in 2020\\n\\t* A multi-lingual version of BERT that can handle text data in multiple languages\\n\\t* Achieved state-of-the-art results on several NLP tasks for multiple languages\\n10. **LLaMA (Large Language Model Applications)**:\\n\\t* Developed by Meta AI in 2021\\n\\t* A highly parameterized language model that achieves state-of-the-art results on several NLP tasks\\n\\t* Designed to be used for a wide range of applications, such as question-answering and text generation\\n11. **MegaMask**:\\n\\t* Developed by Meta AI in 2022\\n\\t* A highly parameterized language model that achieves state-of-the-art results on several NLP tasks\\n\\t* Designed to be used for a wide range of applications, such as question-answering and text generation\\n12. **GPT-3 (Third Generation Pre-trained Language Model)**:\\n\\t* Developed by OpenAI in 2020\\n\\t* A highly parameterized language model that achieves state-of-the-art results on several NLP tasks\\n\\t* Designed to be used for a wide range of applications, such as text generation and question-answering\\n\\nNote that this is not an exhaustive list, and there are many other popular LLMs that have been developed in recent years.'}]\n",
            "Truncated by last 3 chars: [{'role': 'assistant', 'content': 'een developed in recent years.'}]\n",
            "Truncated by 10 words: [{'role': 'assistant', 'content': 'other popular LLMs that have been developed in recent years.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H_MW9F3rcgd"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}